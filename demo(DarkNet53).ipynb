{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "import numpy\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models.yolov3 import *\n",
    "from utils.utils import *\n",
    "from utils.parse_yolo_weights import parse_yolo_weights\n",
    "from utils.vis_bbox import vis_bbox\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose config file for this demo\n",
    "cfg_path = '/home/samsung/Downloads/PyTorch_Gaussian_YOLOv3-master(Darknet)/config/gaussian_yolov3_eval.cfg'\n",
    "\n",
    "# Specify checkpoint file which contains the weight of the model you want to use\n",
    "ckpt_path = '/home/samsung/Downloads/PyTorch_Gaussian_YOLOv3-master(Darknet)/gaussian_yolov3_coco.pth'\n",
    "\n",
    "# Path to the image file fo the demo\n",
    "image_path = '/home/samsung/Downloads/PyTorch_Gaussian_YOLOv3-master(Darknet)/data/gaussian_yolov3/traffic_1.jpg'\n",
    "\n",
    "# Detection threshold\n",
    "detect_thresh = 0.3\n",
    "\n",
    "# Use CPU if gpu < 0 else use GPU\n",
    "gpu = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18571/4136682909.py:3: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  cfg = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "# Load configratio parameters for this demo\n",
    "with open(cfg_path, 'r') as f:\n",
    "    cfg = yaml.load(f)\n",
    "\n",
    "model_config = cfg['MODEL']\n",
    "imgsize = cfg['TEST']['IMGSIZE']\n",
    "confthre = cfg['TEST']['CONFTHRE'] \n",
    "nmsthre = cfg['TEST']['NMSTHRE']\n",
    "gaussian = cfg['MODEL']['GAUSSIAN']\n",
    "\n",
    "# if detect_thresh is not specified, the parameter defined in config file is used\n",
    "if detect_thresh:\n",
    "    confthre = detect_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian YOLOv3\n",
      "Gaussian YOLOv3\n",
      "Gaussian YOLOv3\n",
      "loading checkpoint /home/samsung/Downloads/PyTorch_Gaussian_YOLOv3-master(Darknet)/gaussian_yolov3_coco.pth\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision.models\n",
    "# Load model\n",
    "model = YOLOv3(model_config)\n",
    "\n",
    "# Load weight from the checkpoint\n",
    "print(\"loading checkpoint %s\" % (ckpt_path))\n",
    "state = torch.load(ckpt_path)\n",
    "\n",
    "if 'model_state_dict' in state.keys():\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "else:\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if gpu >= 0:\n",
    "    # Send model to GPU\n",
    "    model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Preprocess image\n",
    "img_raw = img.copy()[:, :, ::-1].transpose((2, 0, 1))\n",
    "img, info_img = preprocess(img, imgsize, jitter=0)  # info = (h, w, nh, nw, dx, dy)\n",
    "img = np.transpose(img / 255., (2, 0, 1))\n",
    "img = torch.from_numpy(img).float().unsqueeze(0)\n",
    "\n",
    "if gpu >= 0:\n",
    "    # Send model to GPU\n",
    "    img = Variable(img.type(torch.cuda.FloatTensor))\n",
    "else:\n",
    "    img = Variable(img.type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samsung/Downloads/PyTorch_Gaussian_YOLOv3-master(Darknet)/models/yolo_layer.py:99: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "  x_shift = dtype(np.broadcast_to(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/samsung/Downloads/PyTorch_Gaussian_YOLOv3-master(Darknet)/demo.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/samsung/Downloads/PyTorch_Gaussian_YOLOv3-master%28Darknet%29/demo.ipynb#ch0000005?line=0'>1</a>\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/samsung/Downloads/PyTorch_Gaussian_YOLOv3-master%28Darknet%29/demo.ipynb#ch0000005?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/samsung/Downloads/PyTorch_Gaussian_YOLOv3-master%28Darknet%29/demo.ipynb#ch0000005?line=2'>3</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(img)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/samsung/Downloads/PyTorch_Gaussian_YOLOv3-master%28Darknet%29/demo.ipynb#ch0000005?line=3'>4</a>\u001b[0m     outputs \u001b[39m=\u001b[39m postprocess(outputs, \u001b[39m80\u001b[39m, confthre, nmsthre)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/samsung/Downloads/PyTorch_Gaussian_YOLOv3-master%28Darknet%29/demo.ipynb#ch0000005?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m outputs[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/PyTorch_Gaussian_YOLOv3-master(Darknet)/models/yolov3.py:170\u001b[0m, in \u001b[0;36mYOLOv3.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_dict[name] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m    169\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         x \u001b[39m=\u001b[39m module(x)\n\u001b[1;32m    171\u001b[0m     output\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/PyTorch_Gaussian_YOLOv3-master(Darknet)/models/yolo_layer.py:119\u001b[0m, in \u001b[0;36mYOLOLayer.forward\u001b[0;34m(self, xin, labels)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# not training\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     pred[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m4\u001b[39m] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride\n\u001b[0;32m--> 119\u001b[0m     pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39;49mview(batchsize, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, n_ch)  \u001b[39m# shsape: [batch, anchor x grid_y x grid_x, n_class + 5]\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgaussian:\n\u001b[1;32m    122\u001b[0m         \u001b[39m# scale objectness confidence with xywh uncertainties\u001b[39;00m\n\u001b[1;32m    123\u001b[0m         sigma_xywh \u001b[39m=\u001b[39m sigma_xywh\u001b[39m.\u001b[39mview(batchsize, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)  \u001b[39m# shsape: [batch, anchor x grid_y x grid_x, 4]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(img)\n",
    "    outputs = postprocess(outputs, 80, confthre, nmsthre)\n",
    "\n",
    "if outputs[0] is None:\n",
    "    print(\"No Objects Deteted!!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "# Visualize detected bboxes\n",
    "coco_class_names, coco_class_ids, coco_class_colors = get_coco_label_names()\n",
    "\n",
    "bboxes = list()\n",
    "classes = list()\n",
    "scores = list()\n",
    "colors = list()\n",
    "sigmas = list()\n",
    "\n",
    "for output in outputs[0]:\n",
    "    x1, y1, x2, y2, conf, cls_conf, cls_pred = output[:7]\n",
    "    if gaussian:\n",
    "        sigma_x, sigma_y, sigma_w, sigma_h = output[7:]\n",
    "        sigmas.append([sigma_x, sigma_y, sigma_w, sigma_h])\n",
    "\n",
    "    cls_id = coco_class_ids[int(cls_pred)]\n",
    "    box = yolobox2label([y1, x1, y2, x2], info_img)\n",
    "    bboxes.append(box)\n",
    "    classes.append(cls_id)\n",
    "    scores.append(cls_conf * conf)\n",
    "    colors.append(coco_class_colors[int(cls_pred)])\n",
    "    \n",
    "    # image size scale used for sigma visualization\n",
    "    h, w, nh, nw, _, _ = info_img\n",
    "    sigma_scale_img = (w / nw, h / nh)\n",
    "    \n",
    "fig, ax = vis_bbox(\n",
    "    img_raw, bboxes, label=classes, score=scores, label_names=coco_class_names, sigma=sigmas, \n",
    "    sigma_scale_img=sigma_scale_img,\n",
    "    sigma_scale_xy=2., sigma_scale_wh=2.,  # 2-sigma\n",
    "    show_inner_bound=False,  # do not show inner rectangle for simplicity\n",
    "    instance_colors=colors, linewidth=3)\n",
    "\n",
    "fig.savefig('trailRunDarkNet.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
